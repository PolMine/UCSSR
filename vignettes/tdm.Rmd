---
title: "Säcke voller Wörter"
subtitle: 'Rezepte zur Term-Dokument-Matrizen'
author: "Andreas Blaette, Christoph Leonhardt"
date: 'Stand:  4. April 2022'
output:
  ioslides_presentation:
    css: css/stylesheet.css
    logo: img/polmine.png
    widescreen: yes
  slidy_presentation: default
editor_options:
  chunk_output_type: console
vignette: >
  %\VignetteIndexEntry{Bag-of-Words}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
bibliography: Literatur_tdm.bib
nocite: '@*'
---

## Vom 'bag-of-words' zur algorithmischen Textanalyse {.smaller}

- In der quantitativen Textanalyse gibt es eine Reihe von Algorithmen, die als Grundlage eine Übersetzung von Texten in sogenannte Term-Dokument-Matrizen erfordern. Dies gilt etwa bei Topic-Modellen, aber auch für viele Verfahren des maschinellen Lernens oder für die in der Politikwissenschaft gängigen Wordscore- und Wordfish-Verfahren.

- Term-Dokument-Matrizen beruhen auf einem sogenannten 'bag-of-words'-Ansatz: Indem ein Text in einen Vektor mit Zählungen von Worten übersetzt wird, wird dessen grammatikalische Struktur und einschließlich der Sequenz des Textes aufgelöst. Ein Term-Dokument-Matrix führt die Vektor-Repräsentation von Texten zusammen, mit den Worten in den Reihen und Dokumenten in den Spalten. Jede Zelle der Matrix gibt an, wie oft Wort i in Dokument j auftritt.

- Technisch müssen Term-Dokument-Matrizen als "dünnbesetzte Matrix" (sparse matrix) realisiert werden, weil bei einem ausdifferenzierten Vokabular bei Weitem nicht jedes Wort in jedem Dokument mindestens einmal auftrifft. Das *polmineR*-Paket nutzt dabei die `TermDocumentMatrix`-Klasse des *tm*-Pakets, die als geringfügige Modifikation aus der `simple_triplet_matrix` des *slam*-Pakets hervorgeht.


## Initialisierung {.smaller}

- Ein Teil der im Folgenden verwendeten Funktionen (Berechnung aller Kookkurrenzen in einem Korpus/einer Partition) sind im *polmineR*-Paket ab Version 0.7.10.9006 enthalten. Bei Bedarf wird die polmineR-Entwicklungsversion installiert.

- Die Beispiele des Foliensatzes basieren auf dem *GermaParl*-Korpus. Der Datensatz ist nach dem Laden von polmineR verfügbar.


```{r initialize, eval = TRUE, message = FALSE}
if (packageVersion("polmineR") < package_version("0.7.10.9006"))
  devtools::install_github("PolMine/polmineR", ref = "dev")
library(polmineR)
```

- Weitere hier verwendete Pakete werden, falls erforderlich, installiert und geladen.

```{r, message = FALSE}
for (pkg in c("magrittr", "slam", "tm", "quanteda", "quanteda.textmodels", "Matrix", "topicmodels")){
  if (!pkg %in% rownames(installed.packages())) install.packages(pkg)
  library(package = pkg, character.only = TRUE)
}
```


## Dünnbesetzte, umwandelbare Matrizen {.smaller}

- Im *polmineR*-Paket stehen die Methoden `as.TermDocumentMatrix()` und `as.DocumentTermMatrix()` zur Verfügung, um Objekte der Klassen `TermDocumentMatrix` oder `DocumentTermMatrix` zu gewinnen.

- Je nachdem, welches Paket für eine weitergehende algorithmische Analyse genutzt wird, können auch Klassen des *Matrix*-Pakets (`sparseMatrix`) oder die *document-feature matrix* (`dfm`) des *quanteda*-Pakets gefordert sein. Der Weg dahin führt über eine einfache Typumwandlung.

- Wichtig für das Verständnis der `TermDocumentMatrix`-Klasse des *tm*-Pakets ist, dass diese letztlich identisch ist mit der `simple_triplet_matrix` des *slam*-Pakets und diesem nur ein Attribut mit der Angabe eines Gewichtungsfaktors hinzufügt. Dies ist grundsätzlich die Term-Frequenz.

- Eine `simple_triplet_matrix` wird definiert über drei Vektoren i, j, v. Der erste gibt die Reihe eines Wertes an, der zweite die Spalte eines Wertes und der dritte den Wert selbst. Indem nur definierte Werte der Matrix angegeben werden, lässt sich der Speicherplatzbedarf gering halten. Bei vielen Dokumenten und einem großen Vokabular könnten Term-Dokument-Matrizen ansonsten schnell riesig und zu große für den verfügbaren Speicher werden!


## Direttissima {.smaller}

- Der einfachste Weg zur Gewinnung einer `DocumentTermMatrix` ist, die `as.DocumentTermMatrix()`-Methode auf ein Korpus anzuwenden. Erforderlich ist nur die Angabe

  - eines p-Attributs (die token sind dann in den Spalten)
  - und eines s-Attributs (die auftretenden s-Attribute sind dann in den Zeilen).

```{r, eval = FALSE}
dtm <- polmineR::as.DocumentTermMatrix("GERMAPARL", p_attribute = "word", s_attribute = "date")
```


## Flexibilität qua `partition_bundle` {.smaller}

- Die beiden zunächst vorgestellten Nutzungsszenarien setzen voraus, dass ein bereits vorhandenes s-Attribut die Gliederung des Korpus / der Partitionen in Dokumente abbildet. Wenn sich die Definition der Dokumente für die Dokument-Term-Matrix erst aus einer Kombination von s-Attributen ergibt, kann die `as.DocumentTermMatrix()`-Methode auch flexibel an einem `partition_bundle` ansetzen, wobei jede erdenkliche Kombination von s-Attributen für die Bildung der `partition`-Objekte im `partition_bundle` herangezogen werden kann.

- Das folgende Szenario illustriert die Verfahrensschritte. Wichtig ist, dass die `partition`-Objekte im `partition_bundle` zunächst um eine Zählung über das für die Zählung der Worthäufigkeiten angereichert werden müssen. Bei der `as.DocumentTermMatrix()`-Methode gibt man dann über das Argument `col` an, aus welcher Spalte (hier: die Zählung) die Zellen der Dokument-Term-Matrix gewonnnen werden.

```{r, eval = FALSE}
bt16 <- partition("GERMAPARL", lp = 16, interjection = FALSE)
bt16_speakers <- partition_bundle(bt16, s_attribute = "speaker", progress = TRUE)
bt16_speakers <- enrich(bt16_speakers, p_attribute = "word", progress = TRUE)
dtm <- polmineR::as.DocumentTermMatrix(bt16_speakers, col = "count")
```


## Mit `as.speeches()` zum `partition_bundle` {.smaller}

```{r, echo = FALSE}
doit <- !file.exists("~/lab/tmp/lda_bt2015speeches_pos.RData")
```

- Im Fall von Plenarprotokollkorpora ist eine plausible Definition der Dokumente, die einer Dokument-Term-Matrix zugrunde liegen, die einzelne Rede von Abgeordneten. Ein Korpus bzw. ein `partition`-Objekt können mit der `as.speeches()`-Methode in ein `partition_bundle` zerlegt werden.

- Diese Zerlegung erfolgt anhand einer Heuristik, nach der als Rede der Beitrag eines Redners an einem Plenartag dient, der höchstens von 500 Worten anderer Redner unterbrochen wird.

- Damit wird ausgeschlossen, dass kurze Unterbrechungen (Zwischenrufe, insbesondere auch Zwischenfragen) den Effekt haben, dass einzelne Redepassagen als eigenständige Reden begriffen werden, obwohl sie der Sache nach eine zusammenhängende Rede darstellen. Zugleich kann erkannt werden, wenn ein Sprecher in einer Sitzung zwei oder mehr verschiedene Reden gehalten hat.

```{r, eval = doit}
bt2015 <- partition("GERMAPARL", year = 2015, interjection = FALSE)
bt2015_speeches <- as.speeches(bt2015, s_attribute_date = "date", s_attribute_name = "speaker")
bt2015_speeches <- enrich(bt2015_speeches, p_attribute = "word")
dtm <- polmineR::as.DocumentTermMatrix(bt2015_speeches, col = "count")
```


## Schrumpfung der Matrix {.smaller}

- Für die meisten Anwendungsszenarien (z.B. Topic Modelling) wird eine gänzlich ungefilterte Matrix unnötig gross sein, den Rechenaufwand unnötig erhöhen und durch "Rauschen" zu verunreinigten Ergebnissen führen. Es empfiehlt sich, eine Bereinigung um seltene Worte vorzunehmen, Rauschen und auch Worte auf einer Stopwort-Liste zu entfernen. 

- Mit dem folgenden ersten Filter-Schritt entfernen wir zunächst Dokumente, die unterhalb einer geforderten Mindestlänge bleiben (hier: 100 Worte). Die Länge des Dokuments ermitteln wir durch Aufsummierung der Häufigkeit der Token in den Reihen (`row_sums`).

```{r, eval = doit}
short_docs <- which(slam::row_sums(dtm) < 100)
if (length(short_docs) > 0) dtm <- dtm[-short_docs,]
```

- In einem zweiten Schritt identifizieren wir Worte, die seltener als 5-mal auftreten (`col_sums`). Diese Worte werden aus der Dokument-Term-Matrix (`dtm`) entfernt.

```{r, eval = doit}
rare_words <- which(slam::col_sums(dtm) < 5)
if (length(rare_words) > 0) dtm <- dtm[,-rare_words]
```


## Weitere Filter-Schritte {.smaller}

- Die `noise()`-Methode des *polmineR*-Pakets unterstützt die Identifikation "rauschiger" Worte in einem Vokabular (Token mit Sonderzeichen, Stopworte). Auch diese werden entfernt.

```{r, eval = doit}
noisy_tokens <- noise(colnames(dtm), specialChars = NULL, stopwordsLanguage = "de")
noisy_tokens_where <- which(unique(unlist(noisy_tokens)) %in% colnames(dtm))
dtm <- dtm[,-noisy_tokens_where]
```

- Nicht erfasst werden dabei Stopwörter, die groß geschrieben wurden, weil sie am Anfang eines Satzes stehen. Diese Fälle erfassen wir gesondert, indem wir eine Stopwort-Liste mit großen Anfangsbuchstaben generieren und anwenden.

```{r, eval = doit}
stopit <- tm::stopwords("de")
stopit_upper <- paste(toupper(substr(stopit, 1, 1)), substr(stopit, 2, nchar(stopit)), sep = "")
stopit_upper_where <- which(stopit_upper %in% colnames(dtm))
if (length(stopit_upper_where) > 0) dtm <- dtm[, -stopit_upper_where]
```


## Berechnung eines Topic Models {.smaller}

- Die durchgeführten Filter-Schritte können dazu führen, dass in der Matrix Dokumente verbleiben, für die aber tatsächlich keinerlei gezählte Token in der Matrix sind. Wir entfernen leere Dokumente, die in der Berechnung Probleme aufwerfen würden.

```{r, eval = doit}
empty_docs <- which(slam::row_sums(dtm) == 0)
if (length(empty_docs) > 0) dtm <- dtm[-empty_docs,]
```

- Genug der Vorarbeit: Wir initiieren die "klassische" Berechnung eines *Latent Dirichlet Allocation*-Topic Models aus dem `topicmodels`-Paket.

```{r, eval = doit}
lda <- topicmodels::LDA(
  dtm, k = 200, method = "Gibbs",
  control = list(burnin = 1000, iter = 3L, keep = 50, verbose = TRUE)
)
```

```{r, echo = FALSE, eval = TRUE}
if (doit == TRUE){
  saveRDS(lda, file = "~/lab/tmp/bt2015speeches_lds.RData")
} else {
  lda <- readRDS(file = "~/lab/tmp/bt2015speeches_lds.RData")
}
```

- Um das Ergebnis zu überprüfen, beziehen wir das Vokabular, welches die einzelnen Topics indiziert. Die Ausgabe erfolgt auf der folgenden Seite.

```{r, echo = TRUE, eval = FALSE}
lda_terms <- terms(lda, 10)
```


## Topic-Term-Matrix {.smaller}

```{r, echo = FALSE, eval = TRUE, message = FALSE}
n_terms <- 5L
lda_terms <- terms(lda, n_terms)
y <- t(lda_terms)
colnames(y) <- paste("Term", 1:n_terms, sep = " ")
DT::datatable(y)
```


## Filtern anhand von Part-of-Speech-Annotationen {.smaller}

```{r, message = FALSE}
pb <- partition("GERMAPARL", year = 2015, interjection = FALSE) %>%
  as.speeches(s_attribute_date = "date", s_attribute_name = "speaker") %>% 
  enrich(p_attribute = c("word", "pos"), progress = TRUE) %>%
  subset(pos == "NN")
```

- An dieser Stelle ist nun noch ein Zwischenschritt erforderlich: Die Spalte mit der Part-of-Speech-Annotation müssen wir "manuell" fallen lassen.

```{r}
pb@objects <- lapply(pb@objects, function(x){x@stat[, "pos" := NULL]; x@p_attribute <- "word"; x})
```


## Das nächste Topic-Modell {.smaller}

```{r, eval = doit, message = FALSE}
dtm <- polmineR::as.DocumentTermMatrix(pb, col = "count")

short_docs <- which(slam::row_sums(dtm) < 100)
if (length(short_docs) > 0) dtm <- dtm[-short_docs,]

rare_words <- which(slam::col_sums(dtm) < 5)
if (length(rare_words) > 0) dtm <- dtm[,-rare_words]

empty_docs <- which(slam::row_sums(dtm) == 0)
if (length(empty_docs) > 0) dtm <- dtm[-empty_docs,]

lda <- topicmodels::LDA(
  dtm, k = 200, method = "Gibbs",
  control = list(burnin = 1000, iter = 3L, keep = 50, verbose = TRUE)
)
```


```{r echo = FALSE}
if (doit == TRUE){
  saveRDS(lda, file = "~/lab/tmp/lda_bt2015speeches_pos.RData")
} else {
  lda <- readRDS(file = "~/lab/tmp/lda_bt2015speeches_pos.RData")
}
```


## Topic-Term-Matrix {.smaller}

```{r, echo = FALSE, eval = TRUE, message = FALSE}
n_terms <- 5L
lda_terms <- terms(lda, n_terms)
y <- t(lda_terms)
colnames(y) <- paste("Term", 1:n_terms, sep = " ")
DT::datatable(y)
```


## Datentransformation: Zur Document-Feature-Matrix I {.smaller}

Es gibt zahlreiche R-Pakete, die computergestütze Textanalysen ermöglichen. Während die meisten Pakete eine Art von Term-Dokumenten-Matrix als Ausgangspunkt nutzen, kann der spezifische Matrixtyp variieren. Methoden des beliebten `quanteda`-Paketes nutzen so eine Document-Feature-Matrix als Input. Mittels Typumwandlung können wir aus `polmineR` ausgegbene Matrizen in eine Document-Feature-Matrix umwandeln.

Erstellen wir zunächst wie zuvor gezeigt ein `partition-bundle`, das wir an dieser Stelle in eine transponierte, 'sparse' Matrix umwandeln.

```{r, eval = TRUE, echo = TRUE, message = FALSE}
pb <- partition("GERMAPARL", lp = 16, interjection = FALSE) %>%
  partition_bundle(s_attribute = "parliamentary_group")
pb <- pb[[names(pb)[!names(pb) %in% c("", "fraktionslos")] ]]
pb <- enrich(pb, p_attribute = "lemma")
dtm <- polmineR::as.sparseMatrix(pb, col = "count")
dtm <- Matrix::t(dtm)
```


## Datentransformation: Zur Document-Feature-Matrix II - Filtern  {.smaller}

Da die Berechnung eines Wordfish Models durchaus speicherintensiv sein kann, filtern wir die erstellte Document-Term-Matrix, um die Datenmenge zu reduzieren. Wir entfernen Stopwords, "noise" (siehe `?noise()`) sowie Worte, die weniger als 10 mal insgesamt vorkommen.

```{r message = FALSE}
noise_to_drop <- polmineR::noise(colnames(dtm), specialChars = NULL, stopwordsLanguage = "de")
noise_to_drop[["stopwords"]] <- c(
  noise_to_drop[["stopwords"]],
  paste(
    toupper(substr(noise_to_drop[["stopwords"]], 1, 1)),
    substr(noise_to_drop[["stopwords"]], 2, nchar(noise_to_drop[["stopwords"]])),
    sep = ""
  )
)

dtm <- dtm[,-which(colnames(dtm) %in% unique(unname(unlist(noise_to_drop))))]

# remove rare words
terms_to_drop_rare <- which(slam::col_sums(dtm) <= 10)
if (length(terms_to_drop_rare) > 0) dtm <- dtm[,-terms_to_drop_rare]
```


## Datentransformation: Zur Document-Feature-Matrix III - Typumwandlung {.smaller}

Eine Document-Feature-Matrix ist intern anders geordnet als unsere Matrix. In folgender Typumwandlung wird die Document-Term-Matrix also in eine Document-Feature-Matrix übersetzt.

```{r, echo = TRUE}
pg_dfm <- new(
  "dfm",
  i = dtm@i,
  p = dtm@p,
  x = dtm@x,
  Dim = dtm@Dim,
  Dimnames = list(
    docs = dtm@Dimnames$Docs,
    features = dtm@Dimnames$Terms
  )
)
```


## Anwendungsfall Wordfish I {.smaller}

Unter anderem bieten das `quanteda.textmodels` Paket eine niedrigschwellige Implementierung von **Wordfish**. Wordfish ist ein bekanntes Modell zur Skalierung politischer Positionen. Hierbei werden Positionen unüberwacht aus Worthäufigkeiten geschlossen. Für einen Überblick über den zugrundeliegenden Algorithmus und eine Auswahl von Veröffentlichungen, die Wordfish nutzen, siehe [hier](http://www.wordfish.org).


## Anwendungsfall Wordfish II {.smaller}

Nun können wir ein erstes Wordfish-Modell berechnen.

```{r fit_wfm, echo = TRUE}
wfm_1 <- quanteda.textmodels::textmodel_wordfish(pg_dfm, c(4, 1))
```

Wir können die `summary()` Methode verwenden, um einen ersten Eindruck darüber zu erlangen, wie das Model die Fraktionen im politischen Raum angeordnet hat.

```{r}
wordfish_summary <- summary(wfm_1)
```


Für die Analyse interessante Parameter sind *theta* und *beta*. Siehe die nächste Folie.

## Anwendungsfall Wordfish II (Fortsetzung) {.smaller}

Theta gibt die Position eines Dokumentes (hier alle Debatten einer Fraktion) an.

```{r}
wordfish_summary$estimated.document.positions
```

Beta beschreibt den Effekt individueller Wörter auf das Scaling.

```{r}
head(wordfish_summary$estimated.feature.scores, 4) # show first 4 terms
```


## Anwendungsfall Wordfish III {.smaller}

Beide Werte können auf unterschiedliche Art und Weise visualisiert werden. Für Theta kann die implementierte Dotplot-Darstellung genutzt werden.

```{r, echo = TRUE}
quanteda.textplots::textplot_scale1d(wfm_1, doclabels = pg_dfm@Dimnames$docs)
```

Hier zeigt sich, dass die Interpretation eines Wordfish-Modells sorgsam vorgegangen werden muss. Auf welcher Skala macht diese Skalierung Sinn?

Hier kann es helfen, sich die Betawerte je Term anzuschauen. 

```{r, echo = FALSE}
betaterm <- data.frame(terms = wfm_1$features, beta = wfm_1$beta)
```

```{r}
head(betaterm[order(betaterm$beta),], 10)
head(betaterm[order(betaterm$beta, decreasing = TRUE),], 10)
```


## Anwendungsfall Wordfish IV {.smaller}

Bekannt sind die sogenannten Eifelturm-Darstellungen, die sich aus dem Wordfish-Modell ergeben. Hierbei wird auf der x-Achse abgetragen, inwiefern ein Wort auf die Skalierung aufläd. Auf der y-Achse wird die geschätzte Wordhäufigkeit **psi** dargestellt. Für die Skalierung spielen vor allem dokumentspezifische und selten vorkommende Terme eine Rolle, während häufig vorkommende und auf Dokumentebene gleichmäßig verteilte Begriffe eine geringe Rolle spielen. 

```{r, echo = TRUE, eval = FALSE}
quanteda.textplots::textplot_scale1d(wfm_1, margin = "features",
                 highlighted = c("Solidität", "Vermögenssteuer", "Klimakiller",
                                "Industriestandort", "Freiheit", "Solidarität"))
```

Die Abbildung wird auf der nächsten Folie angezeigt.


## Anwendungsfall Wordfish V {.smaller}


```{r, echo = FALSE, eval = TRUE}
quanteda.textplots::textplot_scale1d(wfm_1, margin = "features",
                 highlighted = c("Solidität", "Vermögenssteuer", "Klimakiller",
                                "Industriestandort", "Freiheit", "Solidarität"))
```


## Fazit {.smaller}

Ob *Term-Dokument-Matrix*, *Dokument-Term-Matrix* oder *Document-Feature-Matrix*, ob *sparse* oder nicht: Die Darstellung von Texten in Matrizen mit Worten auf der einen und Dokumenten auf der anderen Seite ist für eine Vielzahl von Anwendungsbereichen in der computergestützen Textanalyse von enormer Bedeutung. polmineR bietet hierbei die Möglichkeit, Korpora in diese Formen zu überführen und für weitere Analysen nutzbar zu machen. Zu beachten ist, dass in dieser Darstellung Wortzusammenhänge aufgelöst werden. Diese *bag-of-words*-Ansätze stehen so im Gegensatz zum hermeneutisch-interpretativen von beispielsweise Keyword-in-Context-Analysen. Ein triangulatives Vorgehen kann im Sinne der Validierung deshalb nur angeraten werden.

## Literatur {.smaller}
